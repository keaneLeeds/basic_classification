{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Confusion Matrices and ROC Curves\n",
    "\n",
    "Let's do some hands-on exploring.\n",
    "To make sure we don't take things too seriously, today's algorithm is testing if something is a Kangaroo ${\\large 🦘}$ or not ${\\large 🏭}$!\n",
    "\n",
    "![Error Types](https://imgs.xkcd.com/comics/error_types.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## The Confusion Matrix\n",
    "\n",
    "<p>The confusion matrix is a table layout that makes it easier to visualize the performance of a machine learning algorithm's classification ability.</p>\n",
    "<br />\n",
    "\n",
    "<p>\n",
    "$\\begin{aligned}\n",
    "&\\begin{array}{|c|c|c|}\n",
    "\\hline\n",
    "& Actual\\,Positive & Actual\\,Negative\\\\\n",
    "\\hline\n",
    "Predicted\\,Positive & {\\small True\\,Positive} & {\\small False\\,Positive} \\\\\n",
    "\\hline\n",
    "Predicted\\,Negative & {\\small False\\,Negative} & {\\small True\\,Negative} \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\\end{aligned}$\n",
    "</p>\n",
    "<br />\n",
    "\n",
    "You may recognize False Positives as Type I errors and False Negatives as Type 2 errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Terminology\n",
    "\n",
    "One of the more confusing aspects of the confusion matrix is the confusion of terminology that is used interchangeably between tutorials.\n",
    "\n",
    "#### Sensitivity\n",
    "aka \"Recall\" aka \"Hit Rate\" aka \"True Positive Rate\" aka <b>TPR</b>\n",
    "\n",
    "$$\\frac{TP}{TP\\,+\\,FN} = \\frac{is\\,{\\large 🦘}}{is\\,{\\large 🦘}\\,+\\,isn't\\,{\\large 🏭}}$$\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "#### Specificity\n",
    "aka \"Selectivity\" aka \"True Negative Rate\" aka <b>TNR</b>\n",
    "\n",
    "$$\\frac{TN}{TN\\,+\\,FP} = \\frac{is\\,{\\large 🏭}}{is\\,{\\large 🏭}\\,+\\,isn't\\,{\\large 🦘}}$$\n",
    "\n",
    "<br />\n",
    "\n",
    "\n",
    "#### False Positive Rage\n",
    "aka \"Fall Out\" aka <b>FPR</b>\n",
    "\n",
    "$$\\frac{FP}{FP\\,+\\,TN} = \\frac{isn't\\,{\\large 🦘}}{isn't\\,{\\large 🦘}\\,+\\,is\\,{\\large 🏭}}$$\n",
    "\n",
    "<br />\n",
    "\n",
    "#### Accuracy\n",
    "This one doesn't have any weird names\n",
    "\n",
    "$$\\frac{TP\\,+\\,TN}{TP\\,+\\,TN\\,FP\\,+\\,FN} = \\frac{is\\,{\\large 🦘}\\,+\\,is\\,{\\large 🏭}}{is\\,{\\large 🦘}\\,+\\,is\\,{\\large 🏭}\\,+\\,isn't\\,{\\large 🦘}\\,+\\,isn't\\,{\\large 🏭}}$$\n",
    "\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Kangaroo vs Factory Confusion Matrix\n",
    "$\\begin{aligned}\n",
    "&{\\large} \\\\\n",
    "&\\begin{array}{|c|c|c|}\n",
    "\\hline\n",
    " & Actual\\,{\\large 🦘} & Actual\\,{\\large 🏭\\,} \\\\\n",
    "\\hline\n",
    "Predicted\\,{\\large 🦘} & is\\,{\\large 🦘} & isn't\\,{\\large 🦘} \\\\\n",
    "\\hline\n",
    "Predicted\\,{\\large 🏭\\,} & isn't\\,{\\large 🏭} & is\\,{\\large 🏭} \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\\end{aligned}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### The Hands-On Part\n",
    "Follow the steps below to customize our outputs and learn how these variables work together.\n",
    "\n",
    "In the next code block, change the numbers for the different contingency table (aka Confusion Matrix) values, and run the block to see how our calculations change.\n",
    "\n",
    "<i>Running The Code Block:</i> Pressing [Shift + Enter] is power-user mode, but you can also use the \"▶ Run\" button at the top of the screen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% py\n"
    }
   },
   "outputs": [],
   "source": [
    "TP = 1  # True Positive  = is 🦘\n",
    "FP = 2  # False Positive = isn't 🦘\n",
    "TN = 3  # True Negative  = is 🏭\n",
    "FN = 7  # False Negative = isn't 🏭\n",
    "\n",
    "sensitivity=TP/(TP+FN)\n",
    "specificity=TN/(TN+FP)\n",
    "false_positive_rate=FP/(FP+TN)\n",
    "\n",
    "# code to display the resulting confusion matrix and sensitivity/specificity\n",
    "from IPython.display import display, Latex\n",
    "display(Latex(rf'$\\begin{{array}}{{|c|c|c|}} \\hline \\\\ & Actual\\,🦘 & Actual\\,🏭 \\\\ \\hline \\\\ Predicted\\,🦘 & {TP} & {FP} \\\\ \\hline \\\\ Predicted\\,🏭 & {FN} & {TN} \\\\ \\hline \\end{{array}}$'))\n",
    "display(Latex(rf'$Sensitivity = \\dfrac{{{TP}}}{{{TP+FN}}} = {sensitivity}$'))\n",
    "display(Latex(rf'$Specificity = \\dfrac{{{TN}}}{{{TN+FP}}} = {specificity}$'))\n",
    "display(Latex(rf'$Accuracy = \\dfrac{{{FP}}}{{{FP+TN}}} = {false_positive_rate}$'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Extra Credit:</i> What would happen to <i>accuracy</i> if we made FP really high? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## The Receiver Operating Characteristic and Area Under Curve\n",
    "\n",
    "A receiver operating characteristic curve (ROC curve), is a graphical plot that illustrates the diagnostic ability of a binary classifier system as it's discrimination threshold is varied.\n",
    "\n",
    "The ROC curve is created by plotting the true positive rate (TPR aka \"is 🦘 ratio\") against the false positive rate (FPR aka \"isn't 🦘 ratio\") at various threshold settings.\n",
    "\n",
    "<br />\n",
    "\n",
    "### Sensitivity (FPR) & 1 - Specificity (TPR) equations\n",
    "$$True\\,Positive\\,Rate = Sensitivity = TPR = \\frac{TP}{TP+FN} = \\frac{is\\,{\\large 🦘}}{is\\,{\\large 🦘}\\,+\\,isn't\\,{\\large 🏭}}$$\n",
    "\n",
    "<br />\n",
    "\n",
    "$$False\\,Positive\\,Rate = FPR = 1 - Specificity = \\frac{FP}{TN+FP} = \\frac{isn't\\,{\\large 🦘}}{is\\,{\\large 🏭}\\,+\\,isn't\\,{\\large 🦘}}$$\n",
    "\n",
    "The equations in the next code block will use the values you set up above to calculate the TPR and FPR, but you can set them manually if you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% py\n"
    }
   },
   "outputs": [],
   "source": [
    "TPR = TP/(TP+FN) # True Positive Rate\n",
    "FPR = FP/(TN+FP) # False Positive Rate\n",
    "\n",
    "display(Latex(rf'$True\\,Positive\\,Rate = \\dfrac{{{TP}}}{{{TP+FN}}} = {TPR}$'))\n",
    "display(Latex(rf'$False\\,Positive\\,Rate = \\dfrac{{{FP}}}{{{TN+FP}}} = {FPR}$'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### The Threshold value\n",
    "\n",
    "When our machine learning model is given an input, it generates a number (always between 0.0 and 1.0) that represents its belief in how likely it is that the input is a kangaroo 🦘.\n",
    "\n",
    "The threshold value is one that we choose, and it determines which guesses we think are good enough. Where we set that threshold determines the sensitivity and specificity of the model.\n",
    "\n",
    "The ROC curve is a graphical representation of the sensitivity and specificity for a range of possible threshold values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### The Hands-On Part\n",
    "\n",
    "Set a threshold value below and run the cell to see the resulting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "threshold = 0.70  # the cutoff point for 🦘\n",
    "\n",
    "# code for displaying the ROC curve\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, RocCurveDisplay, auc\n",
    "from jupyterthemes import jtplot\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "jtplot.style(context='poster', fscale=1.4, gridlines='--')\n",
    "\n",
    "y_truth = ['🦘', '🦘', '🦘', '🦘', '🦘', '🦘', '🦘', '🦘', '🦘', '🦘', '🏭', '🦘', '🏭', '🦘', '🏭', '🦘', '🏭', '🦘', '🏭', '🦘', '🏭', '🏭', '🏭', '🏭', '🏭', '🏭', '🏭', '🏭', '🏭', '🏭']\n",
    "predictions = [ 0.95, 0.925, 0.90, 0.875, 0.85, 0.825, 0.80, 0.775, 0.750, 0.725, 0.7, 0.675, 0.65, 0.625, 0.6, 0.575, 0.55, 0.525, 0.50, 0.475, 0.45, 0.425, 0.4, 0.375, 0.35, 0.3, 0.275, 0.25, 0.225, 0.2 ]\n",
    "choice_at_threshold = ['🦘' if i >= threshold else '🏭' for i in predictions]\n",
    "true_kangaroo = [i == j and i == '🦘' for i, j in zip(y_truth, choice_at_threshold)]\n",
    "true_factory = [i == j and i == '🏭' for i,j in zip(y_truth, choice_at_threshold)]\n",
    "false_kangaroo = [i != j and i == '🦘' for i,j in zip(y_truth, choice_at_threshold)]\n",
    "false_factory = [i != j and i == '🏭' for i,j in zip(y_truth, choice_at_threshold)]\n",
    "true_kangaroo_ratio = np.sum(true_kangaroo)/(np.sum(true_kangaroo)+np.sum(false_factory))\n",
    "false_kangaroo_ratio = np.sum(false_kangaroo)/(np.sum(true_factory)+np.sum(false_kangaroo))\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_truth, predictions, pos_label='🦘')\n",
    "display = RocCurveDisplay(fpr, tpr, roc_auc=auc(fpr, tpr))\n",
    "display.plot()\n",
    "plt.plot(false_kangaroo_ratio, true_kangaroo_ratio, 'yo', label='point for threshold choice')\n",
    "plt.title(\"Kangaroo vs Factory ROC\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Running the next cell will show an array of threshold values where the first entry will assume everything is a factory 🏭, and the last entry will assume everything is a kangaroo 🦘. Try setting your threshold value to a one of these and see where it lands on the ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Run the cell below to see the confusion matrix for your threshold choice.\n",
    "\n",
    "The confusion matrix will print like this:\n",
    "```python\n",
    "[[TP, FP]\n",
    " [FN, TN]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_truth, choice_at_threshold).ravel()\n",
    "cm = np.asarray([[tp, fp], [fn, tn]])\n",
    "print(f'Confusion matrix at threshold value {threshold}:')\n",
    "print()\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "What happened when we changed the threshold? What real world impact could this have?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}